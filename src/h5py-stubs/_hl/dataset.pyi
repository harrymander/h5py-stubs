from collections.abc import Iterator, Sequence
from typing import Any, Literal, Self, overload, override

import numpy as np
from _typeshed import Incomplete
from h5py import h5d
from h5py._hl.dims import DimensionManager
from h5py._hl.vds import VDSmap
from numpy.typing import DTypeLike, NDArray

from .base import HLObject

MPI: Incomplete

class AbstractView[T: np.generic]:
    def __init__(self, dset: Dataset[Any]) -> None: ...
    def __len__(self) -> int: ...
    @property
    def dtype(self) -> np.dtype[T]: ...
    @property
    def ndim(self) -> int: ...
    @property
    def shape(self) -> tuple[int, ...]: ...
    @property
    def size(self) -> int: ...
    def __getitem__(self, idx: Any) -> NDArray[T]: ...

    # copy=False raises ValueError
    @overload
    def __array__[NewT: np.generic](
        self,
        dtype: type[NewT],
        copy: Literal[True] | None = ...,
    ) -> NDArray[NewT]: ...
    @overload
    def __array__(
        self,
        dtype: DTypeLike,
        copy: Literal[True] | None = ...,
    ) -> NDArray[Any]: ...
    @overload
    def __array__(self, *, copy: Literal[True] | None = ...) -> NDArray[T]: ...

class AsTypeView[T: np.generic](AbstractView[T]):
    def __init__(self, dset: Dataset[Any], dtype: DTypeLike) -> None: ...

class AsStrView(AbstractView[np.object_]):  # TODO: is np.object_ correct type here?
    def __init__(
        self,
        dset: Dataset[Any],
        encoding: str,
        errors: str = ...,
    ) -> None: ...

class FieldsView[T: np.generic](AbstractView[T]):
    def __init__(
        self,
        dset: Dataset[Any],
        prior_dtype: np.dtype,
        names: str | Sequence[str],
    ) -> None: ...

class CollectiveContext:
    def __init__(self, dset: Dataset[Any]) -> None: ...
    def __enter__(self) -> None: ...
    def __exit__(self, *_) -> None: ...

class ChunkIterator:
    def __init__(self, dset: Dataset[Any], source_sel: Incomplete = ...) -> None: ...
    def __iter__(self) -> Self: ...
    def __next__(self) -> tuple[slice, ...]: ...

# Note that Dataset is not actually runtime subscriptable, so parameterised
# types will need to be quoted. Alternatively, users can defined a type alias.
# E.g. in Python 3.12 or later:
#
#     type Dataset[T: np.generic] = "h5py.Dataset[T]"
#
class Dataset[T: np.generic](HLObject):
    def __init__(self, bind: h5d.DatasetID, *, readonly: bool = ...) -> None: ...
    @property
    @override
    def id(self) -> h5d.DatasetID: ...
    @overload
    def astype[NewT: np.generic](self, dtype: type[NewT]) -> AsTypeView[NewT]: ...
    @overload
    def astype(self, dtype: DTypeLike) -> Self | AsTypeView[Any]: ...

    #
    def asstr(self, encoding: str = ..., errors: str = ...) -> AsStrView: ...

    #
    @overload
    def fields[NewT: np.generic](
        self,
        names: str | Sequence[str],
        *,
        _prior_dtype: type[NewT],
    ) -> FieldsView[NewT]: ...
    @overload
    def fields(
        self,
        names: str | Sequence[str],
        *,
        _prior_dtype: DTypeLike,
    ) -> FieldsView[Any]: ...
    @overload
    def fields(self, names: str | Sequence[str]) -> FieldsView[T]: ...

    #
    @property
    def collective(self) -> CollectiveContext: ...
    @property
    def dims(self) -> DimensionManager: ...
    @property
    def ndim(self) -> int: ...
    @property
    def shape(self) -> tuple[int, ...]: ...
    @shape.setter
    def shape(self, shape: tuple[int, ...]) -> None: ...
    @property
    def size(self) -> int: ...
    @property
    def nbytes(self) -> int: ...
    @property
    def dtype(self) -> np.dtype[T]: ...
    @property
    def chunks(self) -> Sequence[int] | None: ...
    @property
    def compression(self) -> str | None: ...
    @property
    def compression_opts(self) -> Any: ...
    @property
    def shuffle(self) -> bool: ...
    @property
    def fletcher32(self) -> bool: ...
    @property
    def scaleoffset(self) -> int | None: ...
    @property
    def external(self) -> list[tuple[str, int, int]] | None: ...
    @property
    def maxshape(self) -> tuple[int | None, ...] | None: ...
    @property
    def fillvalue(self) -> Any: ...
    def iter_chunks(self, sel: Incomplete = ...) -> ChunkIterator: ...
    def refresh(self) -> None: ...
    def flush(self) -> None: ...
    @property
    def is_virtual(self) -> bool: ...
    def virtual_sources(self) -> list[VDSmap]: ...
    def make_scale(self, name: str = ...) -> None: ...
    @property
    def is_scale(self) -> bool: ...

    # resize: if axis provided, size should be int
    @overload
    def resize(self, size: int, axis: int) -> None: ...
    @overload
    def resize(self, size: tuple[int, ...], axis: None = ...) -> None: ...

    # array-like interface
    def __len__(self) -> int: ...
    def len(self) -> int: ...
    def __iter__(self) -> Iterator[NDArray[T]]: ...
    def __setitem__(self, args: Any, val: Any) -> None: ...

    # array indexing
    @overload
    def __getitem__[NewT: np.generic](
        self,
        args: Any,
        new_dtype: type[NewT],
    ) -> NDArray[NewT]: ...
    @overload
    def __getitem__(self, args: Any, new_dtype: DTypeLike) -> NDArray[Any]: ...
    @overload
    def __getitem__(self, args: Any) -> NDArray[T]: ...

    # copy=False raises ValueError
    @overload
    def __array__[NewT: np.generic](
        self,
        dtype: type[NewT],
        copy: Literal[True] | None = ...,
    ) -> NDArray[NewT]: ...
    @overload
    def __array__(
        self,
        dtype: DTypeLike,
        copy: Literal[True] | None = ...,
    ) -> NDArray[Any]: ...
    @overload
    def __array__(self, *, copy: Literal[True] | None = ...) -> NDArray[T]: ...

    # TODO:
    def read_direct(
        self,
        dest: Incomplete,
        source_sel: Incomplete = ...,
        dest_sel: Incomplete = ...,
    ) -> None: ...
    def write_direct(
        self,
        source: Incomplete,
        source_sel: Incomplete = ...,
        dest_sel: Incomplete = ...,
    ) -> None: ...
